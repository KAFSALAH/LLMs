{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98620c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install python-dotenv openai pandas\n",
    "\n",
    "from typing import Dict, List\n",
    "from utilities import (\n",
    "    num_tokens_from_messages,\n",
    "    get_embedding,\n",
    "    get_n_nearest_neighbors,\n",
    "    memoize_to_sqlite,\n",
    ")\n",
    "from f1_utilities import wikipedia_splitter, Section\n",
    "from io import StringIO\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "import os\n",
    "import itertools\n",
    "import tiktoken\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b747c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "MAX_CONTEXT_WINDOW = 4097\n",
    "MINIMUM_RESPONSE_SPACE = 1000\n",
    "MAX_PROMPT_SIZE = MAX_CONTEXT_WINDOW - MINIMUM_RESPONSE_SPACE\n",
    "\n",
    "\n",
    "def ask_embedding_store(\n",
    "    question: str, embeddings: Dict[Section, List[float]], max_documents: int\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Fetch necessary context from our embedding store, striving to fit the top max_documents\n",
    "    into the context window (or fewer if the total token count exceeds the limit)\n",
    "\n",
    "    :param question: The question to ask\n",
    "    :param embeddings: A dictionary of Section objects to their corresponding embeddings\n",
    "    :param max_documents: The maximum number of documents to use as context\n",
    "    :return: GPT's response to the question given context provided in our embedding store\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(question)\n",
    "\n",
    "    nearest_neighbors = get_n_nearest_neighbors(\n",
    "        query_embedding, embeddings, max_documents\n",
    "    )\n",
    "    messages: Optional[List[Dict[str, str]]] = None\n",
    "\n",
    "    base_token_count = num_tokens_from_messages(get_messages([], question), chat_model)\n",
    "    token_counts = [\n",
    "        len(enc.encode(document.text.replace(\"\\n\", \" \")))\n",
    "        for document, _ in nearest_neighbors\n",
    "    ]\n",
    "    cumulative_token_counts = list(itertools.accumulate(token_counts))\n",
    "    indices_within_limit = [\n",
    "        True\n",
    "        for x in cumulative_token_counts\n",
    "        if x <= (MAX_PROMPT_SIZE - base_token_count)\n",
    "    ]\n",
    "    most_messages_we_can_fit = len(indices_within_limit)\n",
    "\n",
    "    context = [x[0] for x in nearest_neighbors[: most_messages_we_can_fit + 1]]\n",
    "\n",
    "    debug_str = \"\\n\".join(\n",
    "        [\n",
    "            f\"{x[0].location}: {x[1]}\"\n",
    "            for x in nearest_neighbors[: most_messages_we_can_fit + 1]\n",
    "        ]\n",
    "    )\n",
    "    #     print(f\"Using {most_messages_we_can_fit} documents as context:\\n\" + debug_str)\n",
    "    messages = get_messages(context, question)\n",
    "\n",
    "    #     print(f\"Prompt: {messages[-1]['content']}\")\n",
    "    result = openai.chat.completions.create(model=chat_model, messages=messages)\n",
    "    return result.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252d3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"f1_2022.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a5eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@memoize_to_sqlite(\"cache.db\")\n",
    "def wikipedia_api_fetch(article_title: str, field: str) -> str:\n",
    "    base_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"titles\": article_title,\n",
    "        \"explaintext\": 1,\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    if \"query\" in data and \"pages\" in data[\"query\"]:\n",
    "        page = list(data[\"query\"][\"pages\"].values())[0]\n",
    "        if field in page:\n",
    "            return page[field]\n",
    "        else:\n",
    "            raise ValueError(f\"Could not find {field} for page {page}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Could not find page {article_title}\")\n",
    "\n",
    "\n",
    "# Loop through the DataFrame and fetch the content of each Grand Prix\n",
    "df[\"Page_Content\"] = df[\"Link\"].apply(lambda x: wikipedia_api_fetch(x, \"extract\"))\n",
    "df[\"Display Title\"] = df[\"Link\"].apply(lambda x: wikipedia_api_fetch(x, \"title\"))\n",
    "sections: List[Section] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a0b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The precedence of points to split on if a section cant be fit in max length\n",
    "split_point_regexes = [r\"\\n==\\s\", r\"\\n===\\s\", r\"\\n====\\s\", r\"\\n\\n\", r\"\\n\"]\n",
    "\n",
    "\n",
    "# Calculate wikipedia content for each row in the data frame\n",
    "for index, row in df.iterrows():\n",
    "    page_content = row[\"Page_Content\"]\n",
    "    for section in wikipedia_splitter(\n",
    "        row[\"Page_Content\"],\n",
    "        row[\"Display Title\"],\n",
    "        token_limit=MAX_CONTEXT_WINDOW,\n",
    "        split_point_regexes=split_point_regexes,\n",
    "    ):\n",
    "        sections.append(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933cc149",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = \"gpt-3.5-turbo\"\n",
    "embedding_enc = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n",
    "enc = tiktoken.encoding_for_model(chat_model)\n",
    "\n",
    "# Calculate the total number of tokens in the Page Content Column\n",
    "print(str(sections[0]))\n",
    "\n",
    "total_tokens = sum([len(embedding_enc.encode(str(section))) for section in sections])\n",
    "\n",
    "# $0.0004 per 1000 tokens\n",
    "cost = total_tokens * (0.0004 / 1000)\n",
    "print(f\"Estimated Cost ${cost:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbaea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings: Dict[Section, List[float]] = {\n",
    "    section: get_embedding(str(section)) for section in sections\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84abbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_messages(context: List[Section], question: str) -> List[Dict[str, str]]:\n",
    "    context_str = \"\\n\\n\".join([f\"Path: {x.location}\\nBody:\\n{x.text}\" for x in context])\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "You will receive a question from the user and some context to help you answer the question.\n",
    "\n",
    "Evaluate the context and provide an answer if you can confidently answer the question.\n",
    "\n",
    "If you are unable to provide a confident response, kindly state that it is the case and explain the reason.\n",
    "\n",
    "Prioritize offering an \"I don't know\" response over conveying potentially false information.\n",
    "\n",
    "The user will only see your response and not the context you've been provided. Thus, respond in precise detail, directly repeating the information that you're referencing from the context.\n",
    "\"\"\".strip(),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "Using the following information as context, I'd like you to answer a question.\n",
    "\n",
    "{context_str}\n",
    "\n",
    "Please answer the following question: {question}\n",
    "\"\"\".strip(),\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506db5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_embedding_store(\"Who came in 2nd at the British Grand Prix in 2022\", embeddings, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba262cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_embedding_store(\"Who won the 2022 Monaco f1 Grand Prix?\", embeddings, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e271f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_embedding_store(\n",
    "    \"What happened in the first lap of the 2022 British Grand Prix?\", embeddings, 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787437af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_embedding_store(\n",
    "    \"Who finished 9th in the French Grand Prix in 2022?\", embeddings, 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea90f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_embedding_store(\"Who won the F1 drivers championship in 2022?\", embeddings, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
