{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94935a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "openai.api_key = config[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226b521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "import pickle\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b39a325",
   "metadata": {},
   "source": [
    "## Load The Movie Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b40e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./movie_plots.csv\"\n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df17c152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrow our data set to 5000 recent American movies (to save money)\n",
    "movies = (\n",
    "    df[df[\"Origin/Ethnicity\"] == \"American\"]\n",
    "    .sort_values(\"Release Year\", ascending=False)\n",
    "    .head(5000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c0ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the movie plots into a list\n",
    "movie_plots = movies[\"Plot\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef607f51",
   "metadata": {},
   "source": [
    "## Generating The Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a944f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    # replace newlines, which can negatively affect performance.\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    return openai.embeddings.create(input=text, model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577abaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model(\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294c1eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = sum([len(enc.encode(plot)) for plot in movie_plots])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b022db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens\n",
    "cost = total_tokens * (0.0004 / 1000)\n",
    "print(f\"Estimated cost ${cost:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc03323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish a cache of embeddings to avoid recomputing\n",
    "# cache is a dict of tuples (text, model) -> embedding, saved as a pickle file\n",
    "\n",
    "# set path to embedding cache\n",
    "embedding_cache_path = \"movie_embeddings_cache2.pkl\"\n",
    "\n",
    "# load the cache if it exists, and save a copy to disk\n",
    "try:\n",
    "    embedding_cache = pd.read_pickle(embedding_cache_path)\n",
    "except FileNotFoundError:\n",
    "    embedding_cache = {}\n",
    "with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "    pickle.dump(embedding_cache, embedding_cache_file)\n",
    "\n",
    "\n",
    "# define a function to retrieve embeddings from the cache if present, and otherwise request via the API\n",
    "def embedding_from_string(\n",
    "    string, model=\"text-embedding-ada-002\", embedding_cache=embedding_cache\n",
    "):\n",
    "    \"\"\"Return embedding of given string, using a cache to avoid recomputing.\"\"\"\n",
    "    if (string, model) not in embedding_cache.keys():\n",
    "        embedding_cache[(string, model)] = get_embedding(string, model)\n",
    "        print(f\"GOT EMBEDDING FROM OPENAI FOR {string[:20]}\")\n",
    "        with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "            pickle.dump(embedding_cache, embedding_cache_file)\n",
    "    return embedding_cache[(string, model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c56b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line actaully generates the embeddings\n",
    "plot_embeddings = [\n",
    "    embedding_from_string(plot, model=\"text-embedding-ada-002\") for plot in movie_plots\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0664763b",
   "metadata": {},
   "source": [
    "## Plot The Embeddings Using Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8917200",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = movies[[\"Title\", \"Genre\"]].to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e78e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nomic import atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fcfdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = atlas.map_embeddings(embeddings=np.array(plot_embeddings), data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d758ad",
   "metadata": {},
   "source": [
    "## Reccommending Movies By Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa9c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from scipy import spatial\n",
    "\n",
    "\n",
    "def distances_from_embeddings(\n",
    "    query_embedding: List[float],\n",
    "    embeddings: List[List[float]],\n",
    "    distance_metric=\"cosine\",\n",
    ") -> List[List]:\n",
    "    \"\"\"Return the distances between a query embedding and a list of embeddings.\"\"\"\n",
    "    distance_metrics = {\n",
    "        \"cosine\": spatial.distance.cosine,\n",
    "        \"L1\": spatial.distance.cityblock,\n",
    "        \"L2\": spatial.distance.euclidean,\n",
    "        \"Linf\": spatial.distance.chebyshev,\n",
    "    }\n",
    "    distances = [\n",
    "        distance_metrics[distance_metric](query_embedding, embedding)\n",
    "        for embedding in embeddings\n",
    "    ]\n",
    "    return distances\n",
    "\n",
    "\n",
    "def indices_of_nearest_neighbors_from_distances(distances) -> np.ndarray:\n",
    "    \"\"\"Return a list of indices of nearest neighbors from a list of distances.\"\"\"\n",
    "    return np.argsort(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f692039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_recommendations_from_strings(\n",
    "    strings,\n",
    "    index_of_source_string,\n",
    "    k_nearest_neighbors=3,\n",
    "    model=\"text-embedding-ada-002\",\n",
    "):\n",
    "    # Get all of the embeddings\n",
    "    embeddings = [embedding_from_string(string) for string in strings]\n",
    "    # get embedding for our specific query string\n",
    "    query_embedding = embeddings[index_of_source_string]\n",
    "    # get distances between our embedding and all other embeddings\n",
    "    distances = distances_from_embeddings(query_embedding, embeddings)\n",
    "    # get indices of the nearest neighbors\n",
    "    indices_of_nearest_neighbors = indices_of_nearest_neighbors_from_distances(\n",
    "        distances\n",
    "    )\n",
    "\n",
    "    query_string = strings[index_of_source_string]\n",
    "    match_count = 0\n",
    "    for i in indices_of_nearest_neighbors:\n",
    "        if query_string == strings[i]:\n",
    "            continue\n",
    "        if match_count >= k_nearest_neighbors:\n",
    "            break\n",
    "        match_count += 1\n",
    "        print(f\"Found {match_count} closest match: \")\n",
    "        print(f\"Distance of: {distances[i]} \")\n",
    "        print(strings[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e16871",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_recommendations_from_strings(movie_plots, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
