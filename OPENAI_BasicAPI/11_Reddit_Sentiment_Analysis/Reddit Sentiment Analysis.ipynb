{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e12d1a2e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Reddit Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7ac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from typing import List, Dict, Generator, Optional\n",
    "\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import praw\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c10be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-3.5-turbo\"\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.environ[\"REDDIT_CLIENT_ID\"],\n",
    "    client_secret=os.environ[\"REDDIT_CLIENT_SECRET\"],\n",
    "    user_agent=f\"script:test:0.0.1 (by u/yourusername)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96de38cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated 1/4/2024\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "    }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = (\n",
    "            4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        )\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\n",
    "            \"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\"\n",
    "        )\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285797f4",
   "metadata": {},
   "source": [
    "## Getting Reddit Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b630436",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_COLUMNS = [\"subreddit\", \"submission_id\", \"score\", \"comment_body\"]\n",
    "# filename, subreddits = \"cities.csv\", [\n",
    "#     \"NYC\",\n",
    "#     \"Seattle\",\n",
    "#     \"LosAngeles\",\n",
    "#     \"Chicago\",\n",
    "#     \"Austin\",\n",
    "#     \"Portland\",\n",
    "#     \"SanFrancisco\",\n",
    "#     \"Boston\",\n",
    "#     \"Houston\",\n",
    "#     \"Atlanta\",\n",
    "#     \"Philadelphia\",\n",
    "#     \"Denver\",\n",
    "#     \"SeattleWa\",\n",
    "#     \"Dallas\",\n",
    "#     \"WashingtonDC\",\n",
    "#     \"SanDiego\",\n",
    "#     \"Pittsburgh\",\n",
    "#     \"Phoenix\",\n",
    "#     \"Minneapolis\",\n",
    "#     \"Orlando\",\n",
    "#     \"Nashville\",\n",
    "#     \"StLouis\",\n",
    "#     \"SaltLakeCity\",\n",
    "#     \"Columbus\",\n",
    "#     \"Raleigh\",\n",
    "# ]\n",
    "\n",
    "# OTHER POTENTIAL SUBREDDITS TO TRY:\n",
    "# filename, subreddits = \"iphone_v_android.csv\", [\"iphone\", \"Android\"]\n",
    "# filename, subreddits = \"startrek_v_starwars.csv\", [\"startrek\", \"StarWars\"]\n",
    "filename, subreddits = (\n",
    "    \"epl_top_8.csv\",\n",
    "    [\n",
    "        \"reddevils\",\n",
    "        \"LiverpoolFC\",\n",
    "        \"chelseafc\",\n",
    "        \"Gunners\",\n",
    "        \"coys\",\n",
    "        \"MCFC\",\n",
    "        \"Everton\",\n",
    "        \"NUFC\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cb65c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for fetching comments from submissions\n",
    "def comment_generator(submission) -> Generator:\n",
    "    # Do not bother expanding MoreComments (follow-links)\n",
    "    for comment in submission.comments.list():\n",
    "        if (\n",
    "            hasattr(comment, \"body\")\n",
    "            and comment.body != \"[deleted]\"\n",
    "            and comment.body != \"[removed]\"\n",
    "        ):\n",
    "            yield (comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5cd0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_comments(\n",
    "    filename: str,\n",
    "    target_comments_per_subreddit: int,\n",
    "    max_comments_per_submission: int,\n",
    "    max_comment_length: int,\n",
    "    reddit: praw.Reddit,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Collect comments from the top submissions in each subreddit.\n",
    "\n",
    "    Cache results at cache_filename.\n",
    "\n",
    "    Return a dataframe with columns: subreddit, submission_id, score, comment_body\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filename, index_col=\"id\")\n",
    "        assert df.columns.tolist() == DF_COLUMNS\n",
    "    except FileNotFoundError:\n",
    "        df = pd.DataFrame(columns=DF_COLUMNS)\n",
    "\n",
    "    # dict like {comment_id -> {column -> value}}\n",
    "    records = df.to_dict(orient=\"index\")\n",
    "\n",
    "    for subreddit_index, subreddit_name in enumerate(subreddits):\n",
    "        print(f\"Processing Subreddit: {subreddit_name}\")\n",
    "\n",
    "        processed_comments_for_subreddit = len(df[df[\"subreddit\"] == subreddit_name])\n",
    "\n",
    "        if processed_comments_for_subreddit >= target_comments_per_subreddit:\n",
    "            print(\n",
    "                f\"Enough comments fetched for {subreddit_name}, continuing to next subreddit.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # `top`` is a generator, grab submissions until we break (within this loop).\n",
    "        for submission in reddit.subreddit(subreddit_name).top(time_filter=\"month\"):\n",
    "            if processed_comments_for_subreddit >= target_comments_per_subreddit:\n",
    "                break\n",
    "\n",
    "            # The number of comments that we already have for this subreddit\n",
    "            processed_comments_for_submission = len(\n",
    "                df[df[\"submission_id\"] == submission.id]\n",
    "            )\n",
    "\n",
    "            for comment in comment_generator(submission):\n",
    "                if (\n",
    "                    processed_comments_for_submission >= max_comments_per_submission\n",
    "                    or processed_comments_for_subreddit >= target_comments_per_subreddit\n",
    "                ):\n",
    "                    break\n",
    "\n",
    "                if comment.id in records:\n",
    "                    print(\n",
    "                        f\"Skipping comment {subreddit_name}-{submission.id}-{comment.id} because we already have it\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                body = comment.body[:max_comment_length].strip()\n",
    "                records[comment.id] = {\n",
    "                    \"subreddit\": subreddit_name,\n",
    "                    \"submission_id\": submission.id,\n",
    "                    \"comment_body\": body,\n",
    "                }\n",
    "\n",
    "                processed_comments_for_subreddit += 1\n",
    "                processed_comments_for_submission += 1\n",
    "\n",
    "            # Once per post write to disk.\n",
    "            print(f\"CSV rewritten with {len(records)} rows.\\n\")\n",
    "            df = pd.DataFrame.from_dict(records, orient=\"index\", columns=DF_COLUMNS)\n",
    "            df.to_csv(filename, index_label=\"id\")\n",
    "\n",
    "    print(\"Completed.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60962b5a",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a85a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ATTEMPTS = 3\n",
    "\n",
    "\n",
    "def generate_prompt_messages(s: str) -> List[Dict]:\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"\n",
    "The following is a comment from a user on Reddit. Score it from -1 to 1, where -1 is the most negative and 1 is the most positive:\n",
    "\n",
    "The traffic is quite annoying.\n",
    "\"\"\".strip(),\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": \"-0.75\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"\n",
    "The following is a comment from a user on Reddit. Score it from -1 to 1, where -1 is the most negative and 1 is the most positive:\n",
    "\n",
    "The library is downtown.\n",
    "\"\"\".strip(),\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": \"0.0\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"\n",
    "The following is a comment from a user on Reddit. Score it from -1 to 1, where -1 is the most negative and 1 is the most positive:\n",
    "\n",
    "Even though it's humid, I really love the summertime. Everything is so green and the sun is out all the time.\n",
    "\"\"\".strip(),\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": \"0.8\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "The following is a comment from a user on Reddit. Score it from -1 to 1, where -1 is the most negative and 1 is the most positive:\n",
    "\n",
    "{s}\n",
    "\"\"\".strip(),\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f56d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnscorableCommentError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    wait_random_exponential,\n",
    "    retry_if_exception_type,\n",
    "    stop_after_attempt,\n",
    ")\n",
    "\n",
    "\n",
    "@retry(\n",
    "    wait=wait_random_exponential(multiplier=1, max=30),\n",
    "    stop=stop_after_attempt(3),\n",
    "    retry=retry_if_exception_type(UnscorableCommentError)\n",
    "    | retry_if_exception_type(openai.APIConnectionError)\n",
    "    | retry_if_exception_type(openai.APIError)\n",
    "    | retry_if_exception_type(openai.RateLimitError),\n",
    "    reraise=True,  # Reraise the last exception\n",
    ")\n",
    "def score_sentiment(s: str, model: str) -> float:\n",
    "    messages = generate_prompt_messages(s)\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    score_response = response.choices[0].message.content.strip()\n",
    "    # This will raise an Attribute Error if the regular expression doesn't match\n",
    "    try:\n",
    "        return float(re.search(r\"([-+]?\\d*\\.?\\d+)\", score_response).group(1))\n",
    "    except AttributeError:\n",
    "        raise UnscorableCommentError(f\"Could not score comment: {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494b25db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_cost_to_score_comments(df, model) -> float:\n",
    "    \"\"\"Estimate the number of tokens in a dataframe's comment_body column\"\"\"\n",
    "    num_tokens = 0\n",
    "    for comment in df[\"comment_body\"]:\n",
    "        num_tokens += num_tokens_from_messages(generate_prompt_messages(comment), model)\n",
    "    return num_tokens * (0.002 / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a1b2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentiments(filename: str, model: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Score sentiments contained in comments in filename.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename, index_col=\"id\")\n",
    "    assert df.columns.tolist() == DF_COLUMNS\n",
    "\n",
    "    print(\n",
    "        f\"Scoring {len(df)} comments will cost approximately ${estimate_cost_to_score_comments(df, model):.2f}.\"\n",
    "    )\n",
    "    records = df.to_dict(orient=\"index\")\n",
    "\n",
    "    for index, item in enumerate(records.items()):\n",
    "        comment_id, comment = item\n",
    "\n",
    "        if not pd.isna(comment[\"score\"]):\n",
    "            print(f\"{comment_id} was already scored. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        body = comment[\"comment_body\"]\n",
    "        try:\n",
    "            score = score_sentiment(body, model=model)\n",
    "        except UnscorableCommentError:\n",
    "            # The score_sentiment method will retry 3 times before letting this error pass through.\n",
    "            # If it does, we will consider this comment un-processable and skip it.\n",
    "            # For other errors, such as APIConnectionError, we will fail completely and let the user know.\n",
    "            continue\n",
    "        print(\n",
    "            f\"\"\"\n",
    "            {comment_id} - ({index + 1} of {len(records)} Comments)\n",
    "            Body: {body[:80]}\n",
    "            Score: {score}\"\"\".strip()\n",
    "        )\n",
    "\n",
    "        records[comment_id][\"score\"] = score\n",
    "        df = pd.DataFrame.from_dict(records, orient=\"index\", columns=DF_COLUMNS)\n",
    "        df.to_csv(filename, index_label=\"id\")\n",
    "\n",
    "    print(\"Scoring completed.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d24a94d",
   "metadata": {},
   "source": [
    "## Plotting Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68561684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_scores_sorted(df):\n",
    "    avg_scores = df.groupby(\"subreddit\")[\"score\"].mean().reset_index()\n",
    "    avg_scores = avg_scores.sort_values(\"score\", ascending=True)\n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed06f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our plotting function\n",
    "\n",
    "\n",
    "# https://seaborn.pydata.org/examples/kde_ridgeplot.html\n",
    "def get_avg_score_by_subreddit(dataframe):\n",
    "    \"\"\"\n",
    "    Given a pandas DataFrame with columns \"subreddit\" and \"score\", returns a new DataFrame\n",
    "    with the average score and standard deviation for each subreddit.\n",
    "    \"\"\"\n",
    "    # Group by subreddit and calculate the mean and standard deviation for each group\n",
    "    subreddit_stats = dataframe.groupby(\"subreddit\")[\"score\"].agg([\"mean\", \"std\"])\n",
    "\n",
    "    # Rename columns to indicate that they represent the mean and standard deviation\n",
    "    subreddit_stats.columns = [\"mean_score\", \"standard_deviation\"]\n",
    "\n",
    "    subreddit_stats = subreddit_stats.sort_values(\"mean_score\", ascending=True)\n",
    "\n",
    "    # Return the new DataFrame\n",
    "    return subreddit_stats\n",
    "\n",
    "\n",
    "def plot_sentiments(df):\n",
    "    sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "    # Create the data\n",
    "    df_scores = df[[\"score\", \"subreddit\"]]\n",
    "\n",
    "    # Initialize the FacetGrid object\n",
    "    pal = sns.cubehelix_palette(10, rot=-0.25, light=0.7)\n",
    "    g = sns.FacetGrid(\n",
    "        df_scores,\n",
    "        row=\"subreddit\",\n",
    "        row_order=get_avg_score_by_subreddit(df_scores).index.to_list(),\n",
    "        hue=\"subreddit\",\n",
    "        aspect=15,\n",
    "        height=0.5,\n",
    "        palette=pal,\n",
    "    )\n",
    "\n",
    "    # Draw the densities in a few steps\n",
    "    g.map(\n",
    "        sns.kdeplot,\n",
    "        \"score\",\n",
    "        bw_adjust=0.5,\n",
    "        clip_on=False,\n",
    "        fill=True,\n",
    "        alpha=1,\n",
    "        linewidth=1.5,\n",
    "    )\n",
    "    g.map(sns.kdeplot, \"score\", clip_on=False, color=\"w\", lw=2, bw_adjust=0.5)\n",
    "\n",
    "    # passing color=None to refline() uses the hue mapping\n",
    "    g.refline(y=0, linewidth=2, linestyle=\"-\", color=None, clip_on=False)\n",
    "\n",
    "    # Define and use a simple function to label the plot in axes coordinates\n",
    "    def label(x, color, label):\n",
    "        ax = plt.gca()\n",
    "        ax.text(\n",
    "            0,\n",
    "            0.2,\n",
    "            label,\n",
    "            fontweight=\"bold\",\n",
    "            color=color,\n",
    "            ha=\"left\",\n",
    "            va=\"center\",\n",
    "            transform=ax.transAxes,\n",
    "        )\n",
    "\n",
    "    g.map(label, \"score\")\n",
    "\n",
    "    # Set the subplots to overlap\n",
    "    g.figure.subplots_adjust(hspace=-0.25)\n",
    "\n",
    "    # Remove axes details that don't play well with overlap\n",
    "    g.set_titles(\"\")\n",
    "    g.set(yticks=[], ylabel=\"\")\n",
    "    g.despine(bottom=True, left=True)\n",
    "\n",
    "    # display(g.fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a67079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bed592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e0682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9200ee55",
   "metadata": {},
   "source": [
    "## Run Everything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182de45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SUBREDDITS = len(subreddits)\n",
    "TARGET_COMMENTS_PER_SUBREDDIT = 50\n",
    "MAX_COMMENTS_PER_SUBMISSION = 10\n",
    "MAX_COMMENT_LENGTH = 2000\n",
    "\n",
    "collect_comments(\n",
    "    filename=filename,\n",
    "    target_comments_per_subreddit=TARGET_COMMENTS_PER_SUBREDDIT,\n",
    "    max_comments_per_submission=MAX_COMMENTS_PER_SUBMISSION,\n",
    "    max_comment_length=MAX_COMMENT_LENGTH,\n",
    "    reddit=reddit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8a244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = score_sentiments(\n",
    "    filename=filename,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9937bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentiments(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d1b752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1acd1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
